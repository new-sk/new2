# 4장 확률론적 학습 나이브 베이즈를 사용한 분류

- 베이즈 분류기
  - 많은 속성을 고려해야 하는 문제에 가장 적합
  - 많은 다른 알고리즘에서는 약한 효과를 내는 속성을 무시하는 경향이 있음
 
 - 상호배타적이고 포괄적
   - 1 = P(spam) + P(ham)
   - p(spam) = P(~ham)  (예사건)

 - 벤다이어그램
   - P(spam) / P(viagra) / P(ham)
   - P(spam ∩ ~viagra) + P(spam ∩ viagra) + P(~spam ∩ viagra)
   - spam 과 viagra가 독립이라면 : P(spam ∩ viagra) = P(spam)P(viagra)

 - 장점
   - 단순하고 빠르며 매우 효과점
   - 노이즈 / 결측 있어도 잘 작동
   - 훈련에 대해 적은 예제 필요, 많을 때도 잘 작동
   - 예측에 대한 추정된 확률 얻기 쉬움

 - 단점
   - 모든 속성은 동등하게 중요하며 독리적이라는 가정
   - 수치 속성 잘 작동 안됨
   - 추청된 확률이 예측된 범주보다 덜 신뢰적

 - 라플라스 추정기
   - 범주 레벨이 전혀 발생하지 않았다면 문제 발생 (예 : 새로운 용어 발생 -> 사후 확률 0 -> 다른 증거들도 무시됨)
   - 1로 설정 

 - 수치 속성 -> 구간화하여 사용 (binning)
   - cut point, 잘 모를 때는 4분위수 활용 가능

 - 텍스트 데이터 전처리 (tm package)
   - install.packages("tm")
   - sms_corpus <- Corpus (VectorSource(sms_raw$text))
   - 휘발성 저장장치(VCorpus) / 영구 저장장치(PCorpus)
   - inspect
   - DocumentTermMatrix : 희소 매트릭스 생성

 - 시각화 : word clound
   - install.packages("wordcloud")
   - wordcloud(corpus, min_freq = 40, random.order = F)
     - corpus 데이터에서 40번 이상 나타난 단어를 빈도순에 따라 가운데에서 차례로 생성
   - wordcloud(corpus, max_words = 40, scale = c(3,0.5))
     - 40개만 표시하고, 글자의 크기는 0.5~3 사이의 크기
   - subset (dataset, type == "spam")

 - M <- naiveBayes(train, class, laplace=0)
   - 훈련데이터셋, 결과 팩터, 라플라스값은 0으로 초기화